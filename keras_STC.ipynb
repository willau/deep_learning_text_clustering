{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Text Clustering using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following work is mainly inspired by:\n",
    "- 2014 - Convolutional Neural Networks for Sentence Classification\n",
    "- 2014 - A Convolutional Neural Network for Modelling Sentences\n",
    "- 2017 - Self-Taught Convolutional Neural Networks for Short Text Clustering\n",
    "\n",
    "<br>\n",
    "Word embedding from fasttext are distributed under the license:\n",
    "- Creative Commons Attribution-Share-Alike License 3.0 (free commercial use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook\n",
    "from webcolors import name_to_rgb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer, normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Flatten, Reshape, merge\n",
    "from keras.layers import Dense, Conv1D, Dropout, GlobalMaxPooling1D\n",
    "\n",
    "# self-made function\n",
    "from utils.target import laplacian_eigenmaps, binarize\n",
    "from utils.metrics import map_label, cluster_quality\n",
    "from utils.embedding import Embeddor\n",
    "from utils.variable import COLOR_NAMES\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short text datasets\n",
      "['Biomedical.txt',\n",
      " 'Biomedical_gnd.txt',\n",
      " 'SearchSnippets.txt',\n",
      " 'SearchSnippets_gnd.txt',\n",
      " 'StackOverflow.txt',\n",
      " 'StackOverflow_gnd.txt']\n",
      "\n",
      "Embedding\n",
      "['glove100K.100d.vec', 'wiki.en.vec', 'wiki.en.vec.crdownload']\n"
     ]
    }
   ],
   "source": [
    "EMBDIR = \"./embeddings/\"\n",
    "DATADIR = \"./data/short_texts/\"\n",
    "\n",
    "print('\\nShort text datasets')\n",
    "pprint(os.listdir(DATADIR))\n",
    "\n",
    "print('\\nEmbedding')\n",
    "pprint(os.listdir(EMBDIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pre-trained Fasttext word embedding\n",
    "\n",
    "Pre-trained word vectors for English languages, trained on Wikipedia using fastText. These vectors of dimension `300` were obtained using the skip-gram model described in `Bojanowski et al.(2016)` with default parameters.\n",
    "\n",
    "We extracted the `100 000` most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: glove100k.100d.vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76389b41447646f2b4ae9ea92cd29cf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding shape: (100001, 100)\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_emb = int(1 * 1e5)\n",
    "# emb_name = 'fasttext.wiki.en.vec'\n",
    "emb_name = 'glove100k.100d.vec'\n",
    "print(\"Embedding: %s\" % emb_name)\n",
    "src_emb = os.path.join(EMBDIR, emb_name)\n",
    "\n",
    "# embeddor\n",
    "embeddor = Embeddor(notebook_display=True)\n",
    "emb_mat = embeddor.load_emb(src_emb, n_emb)\n",
    "print(\"Embedding shape: {}\".format(emb_mat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('napoleon', 0.99999976),\n",
       " ('bonaparte', 0.87043333),\n",
       " ('napol√©on', 0.66390216),\n",
       " ('napoleonic', 0.65167707),\n",
       " ('augustus', 0.64356035),\n",
       " ('caesar', 0.64281428),\n",
       " ('1812', 0.63069737),\n",
       " ('tsar', 0.6110847),\n",
       " ('xiv', 0.61083198),\n",
       " ('emperor', 0.60819435)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddor.most_similar('napoleon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 20,000 short texts\n"
     ]
    }
   ],
   "source": [
    "#DATA = \"Biomedical\"\n",
    "#DATA = \"SearchSnippets\"\n",
    "DATA = \"StackOverflow\"\n",
    "\n",
    "text_path = os.path.join(DATADIR, '%s.txt' % DATA)\n",
    "with open(text_path, encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "label_path = os.path.join(DATADIR, '%s_gnd.txt' % DATA)\n",
    "with open(label_path, encoding=\"utf-8\") as f:\n",
    "    target = f.readlines()\n",
    "target = [int(label.rstrip('\\n')) for label in target]\n",
    "    \n",
    "print(\"Total: %s short texts\" % format(len(data), \",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3,000 samples\n"
     ]
    }
   ],
   "source": [
    "n = len(data)\n",
    "sample = 3000\n",
    "print(\"Loading %s samples\" % format(sample, \",\"))\n",
    "index_sample = np.random.randint(0, n, sample)\n",
    "\n",
    "data = [data[idx] for idx in index_sample]\n",
    "target = [target[idx] for idx in index_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3943 unique tokens.\n",
      "Average length: 8\n",
      "Max length: 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences_full = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "MAX_NB_WORDS = len(word_index)\n",
    "\n",
    "seq_lens = [len(s) for s in sequences_full]\n",
    "print(\"Average length: %d\" % np.mean(seq_lens))\n",
    "print(\"Max length: %d\" % max(seq_lens))\n",
    "MAX_SEQUENCE_LENGTH = max(seq_lens)\n",
    "\n",
    "X = pad_sequences(sequences_full, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = target\n",
    "tfidf = tokenizer.sequences_to_matrix(sequences_full, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 2841 words in the embedding matrix\n"
     ]
    }
   ],
   "source": [
    "# creating embedding matrix\n",
    "EMBEDDING_DIM = embeddor.emb_dim\n",
    "N_CLASSES = 21\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding = embeddor.get_emb(word)\n",
    "    if embedding is not None:\n",
    "        embedding_matrix[i] = embedding\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dictionary containing all target vectors\n",
    "Y = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average embeddings (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of average embedding:  (3000, 100)\n",
      "CPU times: user 200 ms, sys: 69.7 ms, total: 270 ms\n",
      "Wall time: 261 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "denom = 1 + np.sum(tfidf, axis=1)[:, None]\n",
    "normed_tfidf = tfidf/denom\n",
    "average_embeddings = np.dot(normed_tfidf, embedding_matrix)\n",
    "Y[\"ae\"] = average_embeddings\n",
    "print(\"Shape of average embedding: \", Y['ae'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of latent semantic vectors:  (3000, 100)\n",
      "CPU times: user 1.85 s, sys: 169 ms, total: 2.02 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsa = make_pipeline(\n",
    "    TruncatedSVD(100),\n",
    "    Normalizer(copy=False),\n",
    ") \n",
    "lsa_vectors = lsa.fit_transform(tfidf)\n",
    "Y[\"lsa\"] = lsa_vectors\n",
    "print(\"Shape of latent semantic vectors: \", Y['lsa'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian Eigenmaps (LE) [memory intensive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting nearest neighbors\n",
      "Creation of heat kernel affinity matrix\n",
      "Spectral embedding\n",
      "(3000, 15)\n",
      "CPU times: user 7.84 s, sys: 673 ms, total: 8.51 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# eigen vectors from graph of text similarity\n",
    "laplacian_vectors = laplacian_eigenmaps(lsa_vectors, n_neighbors=15, subdim=15, n_jobs=-1)\n",
    "Y[\"le\"] = laplacian_vectors\n",
    "print(Y['le'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 15)\n",
      "[ 1.  1.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "reduction_name = \"le\"\n",
    "B = binarize(Y[reduction_name])\n",
    "\n",
    "# Last dimension in the CNN\n",
    "TARGET_DIM = B.shape[1]\n",
    "\n",
    "# Example of binarized target vector\n",
    "print(B.shape)\n",
    "print(B[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for deep feature representation learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 34, 100)           394400    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34, 100)           50100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                1515      \n",
      "=================================================================\n",
      "Total params: 446,015\n",
      "Trainable params: 51,615\n",
      "Non-trainable params: 394,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_matrix_copy = embedding_matrix.copy()\n",
    "trainable_embedding = False\n",
    "\n",
    "# Embedding layer\n",
    "pretrained_embedding_layer = Embedding(\n",
    "    input_dim=MAX_NB_WORDS+1,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "# Input\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
    "\n",
    "# 1st Layer\n",
    "x = Conv1D(100, 5, activation='tanh', padding='same')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(TARGET_DIM, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, predictions)\n",
    "\n",
    "# Fine-tune embeddings or not\n",
    "model.layers[1].trainable=trainable_embedding\n",
    "\n",
    "# Loss and Optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/1\n",
      "2s - loss: 0.6861 - mean_absolute_error: 0.4918 - val_loss: 0.6736 - val_mean_absolute_error: 0.4844\n",
      "CPU times: user 6.38 s, sys: 519 ms, total: 6.9 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb_epoch = 1\n",
    "model.fit(X, B, validation_split=0.2,\n",
    "          epochs=nb_epoch, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep feature representations h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (3000, 100)\n"
     ]
    }
   ],
   "source": [
    "# take the penultimate layer\n",
    "input = model.layers[0].input\n",
    "output = model.layers[-2].output\n",
    "beheaded_model = Model(input, output)\n",
    "H = beheaded_model.predict(X)\n",
    "print(\"Sample shape: {}\".format(H.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for evaluating cluster quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "true_labels = y\n",
    "n_clusters = len(np.unique(y))\n",
    "print(\"Number of classes: %d\" % n_clusters)\n",
    "km = KMeans(n_clusters=n_clusters, n_jobs=-1)\n",
    "result = dict()\n",
    "pred = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means on deep feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3000, 100)\n"
     ]
    }
   ],
   "source": [
    "deep_features = normalize(H, norm='l2')\n",
    "print('Shape: {}'.format(deep_features.shape)) \n",
    "km.fit(deep_features)\n",
    "pred['deep'] = km.labels_\n",
    "result['deep'] = cluster_quality(true_labels, pred['deep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means on LSA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_features = Y['lsa']\n",
    "print('Shape: {}'.format(lsa_features.shape)) \n",
    "km.fit(lsa_features)\n",
    "pred['lsa'] = km.labels_\n",
    "result['lsa'] = cluster_quality(true_labels, pred['lsa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means on laplacian eigenvalues aka Spectral clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_features = normalize(Y['le'], norm='l2')\n",
    "print('Shape: {}'.format(laplacian_features.shape)) \n",
    "km.fit(laplacian_features)\n",
    "pred['eigen'] = km.labels_\n",
    "result['eigen'] = cluster_quality(true_labels, pred['eigen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random sample because t-sne is slow\n",
    "tsne_sample = 1000\n",
    "index_sample = np.random.randint(low=0, high=sample, size=(tsne_sample,))\n",
    "\n",
    "# mapping color to sample\n",
    "labels = np.unique(y)\n",
    "colormap = {label: color for label, color in zip(labels, COLOR_NAMES)}\n",
    "colors = [colormap[x] for x in true_labels]\n",
    "color_rgb = [name_to_rgb(name) for name in colors]\n",
    "color_rgb_norm = [(x/255., y/255., z/255.) for x, y, z in color_rgb]\n",
    "color_sample = [color_rgb_norm[idx] for idx in index_sample]\n",
    "\n",
    "# plot function\n",
    "def plot_tsne(T, color):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(T[:, 0], T[:, 1], c=color, s=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE \n",
    "\n",
    "<br>\n",
    "Remarks:\n",
    "- Using Laplacian Eigenvalues, we obtain very dense clusters\n",
    "- Using LSA as target, results are also quite good\n",
    "- LSA is very competitive for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T-SNE model\n",
    "tsne = TSNE(n_components=2, \n",
    "            perplexity=30, \n",
    "            early_exaggeration=4.,\n",
    "            learning_rate=1000,\n",
    "            init=\"pca\", \n",
    "            metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE on deep feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rand_deep_features = deep_features[index_sample]\n",
    "tsne_deep = tsne.fit_transform(rand_deep_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(tsne_deep, color=color_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE on LSA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne_lsa = tsne.fit_transform(lsa_features[index_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib visualization\n",
    "plot_tsne(tsne_lsa, color=color_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### t-SNE on laplacian eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne_laplacian = tsne.fit_transform(laplacian_features[index_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_tsne(tsne_laplacian, color=color_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "9d564caaad174afd811c294e4b1919f9": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
